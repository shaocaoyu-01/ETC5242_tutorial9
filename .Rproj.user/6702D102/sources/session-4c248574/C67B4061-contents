---
title: "Week 9, Notes & Exercises"
subtitle: "Statistical Thinking (ETC2420 / ETC5242)"
date-format: "[Semester 2,] YYYY"
date: "2025"
toc: true
format:
  html:
    embed-resources: true
---

```{r}
#| label: setup
#| include: false
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(broom)
```

This week we are looking at simple linear regression and model
selection/evaluation using Olympic Medal data from Rio and London, using medal
counts from the London 2012 Summer Olympic games to predict medal counts from
the Rio 2016 Summer Olympic games.

Before you begin, you'll need to download the following two data files from
Moodle:

-   `rio_olympics2016.csv`
-   `london_olympics2012.csv`

In 2016, 205 teams competed for 973 medals, but only 86 teams scored at least
one medal.\
In 2012, 204 teams participated for 962 medals, with 85 teams receiving at
least one medal.

Our analysis will focus on teams that win medals, though there are many teams
that compete in the Olympics that do not win any medals. It is important to
keep this fact in mind as you interpret your results.[^1]

[^1]: One might prefer to analyse data that has the additional zero medal
    counts included in the dataset. However, statistically the incorporation of
    the large number of zero values requires more complex models than what we
    have covered in this unit.

# Preparing and exploring the data

To start with, we want to load the data and do some preliminary exploration,
especially some basic visualisations. The goal is to understand what the data
represent, and spot any features or issues (e.g., unexpected values or missing
values). This is important to do before any formal statistical analysis.

Normally we should produce both visual and numerical summaries to illustrate
and communicate the main features of the data.

***Exercises 1--3***

1.  Read in the data files, into tibbles called `data2012` and `data2016`.


```{r}
data2012 <- read_csv("data/london_olympics2012.csv")
data2016 <- read_csv("data/rio_olympics2016.csv")
```

2.  Create bar plots of the total medal count for both datasets (2012 and
    2016).
    
```{r}
library(ggplot2)

ggplot(data2012, aes(x = reorder(Country, Total), y = Total)) +
  geom_col(width = 0.7) +
  coord_flip() +
  labs(title = "Figure 1. Medal counts", x = NULL, y = "Total medals") +
  theme_minimal(base_size = 12)

```


3.  Describe the distribution of the total medal counts. What is the main
    noticeable feature?
    


## Joining data

Before we can fit a regression model that uses data from both 2012 and 2016, we
need to put the data together into a single tibble. One way to do this is with
the `full_join()` function (from the `dplyr` package).

What we need is the medal count from both years, using the team (country) name
to identify which ones should be paired together. The following R code will do
that, creating a new tibble called `oly`.

```{r}
#| eval: false
data2012_prejoin <- data2012 |>
  select(Country, Total) |>
  rename(Total_2012 = Total)

data2016_prejoin <- data2016 |>
  select(Country, Total) |>
  rename(Total_2016 = Total)

oly <- full_join(data2012_prejoin,
                 data2016_prejoin,
                 by = "Country")
```

Notice that we have only taken the `Country` and `Total` columns, and we have
told R to match the data using `Country` (via the `by = "Country"` argument).

Now take a look at the `oly` tibble. Did the `full_join()` function work as
expected?

***Exercise 4***

4.  Can you spot any problems with the `oly` tibble produced using
    `full_join()`?

An alternative way to merge data is with the `inner_join()` function. It will
only keep observations that appear in **both** of the original tibbles, whereas
`full_join()` will keep all observations.

**Warning:** For most analyses, we don't want to accidentally "lose" any data,
which means that `inner_join()` is often not appropriate to use. However, in
today's exercise this is exactly what we need. Some other alternatives include
`left_join()` and `right_join()`; read more about these on the help page:
`help("mutate-joins")`.

The following code chunk uses `inner_join()`.

```{r}
#| eval: false
oly <- inner_join(data2012_prejoin,
                  data2016_prejoin,
                  by = "Country")
```

Now that we have merged the data, we can explore them further.

***Exercises 5--6***

5.  Draw a scatter plot of the total medal counts, comparing 2016 with 2012.

6.  Describe the relationship you can see on the scatter plot.

# Fitting a simple linear regression

Let's use the 2012 total for each team as a predictor for the 2016 total, with
a simple linear regression model:

$$y_i = \beta_0 + \beta_1 x_i + \varepsilon_i.$$

The regression model describes how the response variable $y_i$ (the 2016 medal
count) changes in relation to a change in the predictor $x_i$ (the 2012 medal
count), **on average**. This means that we don't expect the data points to fall
exactly on the regression line, but should appear to be randomly scattered (in
the vertical direction) around the line.

***Exercise 7***

7.  Use the `lm()` function (as demonstrated in the lectures) to fit the above
    model. Store your result in a variable called `oly_lm`.

To look at the details of the fitted model, we recommend the following
functions which both come from the `broom` package:

-   `tidy()`, which shows details about the regression coefficients.
-   `glance()`, which shows some overall model summaries.

Try them out:

```{r}
#| eval: false
glance(oly_lm)
```

```{r}
#| eval: false
tidy(oly_lm)
```

Based on the output of the above commands (once you have run them), we can
report the fitted model as: $\hat{y} = 0.99 + 0.93 x$

Here's how we can interpret the various quantities:

-   The *fitted line* or *"line of best fit"*:
    $\hat{y} = \hat\beta_0 + \hat\beta_1 x$.\
    Note that we don't write an error or residual term here. This equation
    represents the regression **line**.

-   $\hat\beta_1$ is the slope of the fitted line. It represents the average
    number of extra medals we expect in 2016 for each extra medal that was
    gained in 2012.

-   $\hat\beta_0$ is the intercept of the fitted line with y-axis. It
    represents the estimated number of medals that we expect for a team in 2016
    that had not won any medals in 2012.\
    **Note:** given we have excluded from our data any teams that did not win
    medals, this interpretation is an *extrapolation* outside the range of our
    data, and thus may not be reliable.

## Variation explained

Does the model do a good job of modelling the data?

One quantitive measure of this is the *coefficient of determination*, also
known as $R^2$ or R-squared. This can be interpreted as the proportion of
variation in the response variable that is "explained" by the regression line.
It can be computed as
$$R^2 = 1- \frac{\sum_{i=1}^n e_i^2}{\sum_{i=1}^n (y_i-\bar{y})^2},$$ where
$e_i$ is the $i$th residual. However, we don't need to compute this ourselves
because we can simply extract it from the `oly_lm`, as follows:

```{r}
#| eval: false
glance(oly_lm)$r.squared
```

This output (once you have run it) tells us that about 93% of the variation in
the 2016 medal tally can be explained by the regression model (i.e., predicted
by the 2012 medal tally). This is very close to 1, indicating a very good fit.
This is not surprising given what we saw earlier on the scatter plot.

A high R-squared value does not guarantee a good model---it just tells us that
the regression captures most of the variation in $y$.

We need to be sure that the model doesn't have any major problems with it
first. We can visually check the model using residual plots, and we can check
whether the fitted model is capturing patterns from the whole dataset or if it
is mostly influenced by a small number of observations.

## Visual diagnostics

In a regression model, we want to explain any relationship between the
predictor and response variable. If we have done a good job, then what is left
over (the residuals) should look like "random scatter", with no discernible
patterns.

Let's look at whether the model is a good fit to the data. First, we can simply
add the fitted line to our scatter plot.

***Exercise 8***

8.  Draw the scatter plot again, this time adding the fitted line.

Next, we can look at a set of standard diagnostic plots, from which we can see
if there are any systematic patterns in the residuals. The following command
will automatically create such plots:

```{r}
#| eval: false
plot(oly_lm, 1:2)
```

***Exercise 9***

9.  Describe the distribution of the residuals. Do you notice any patterns or
    distinctive features?

If we want to access the residuals or fitted values directly, the following
functions are available:

```{r}
#| eval: false
residuals(oly_lm)
fitted(oly_lm)
```

For example, we can create our own plot of residuals vs fitted values:

```{r}
#| eval: false
ggplot() +
  aes(x = fitted(oly_lm), y = residuals(oly_lm)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0, lty = 2) +
  theme_bw()
```

***Exercises 10--11***

10. Draw your own version of the QQ plot of the residuals (versus the
    corresponding theoretical normal quantiles), and also a histogram.

11. For a good fit, the distribution of residuals should be approximately
    symmetric and bell-shaped. Is that what you can see?

## Potential influential observations

We are also concerned about potentially overly influential observations. In
this case, we know that the USA won the most medals in 2016, and looking at the
residual plots produced above it seems that the USA also has one of the largest
residuals. So let's first explore what happens if we excluded the USA and fit
the model again.

***Exercises 12--17***

12. Create a new tibble that is the same as `oly` but with the USA excluded.

13. Fit the regression model to the new tibble. How do the regression
    coefficients compare to the original model?

14. What is the $R^2$ of this new model?

15. Draw diagnostic plots for this new model.

16. Draw a scatter plot of the data, overlayed with the fitted lines from both
    models.

17. What can you conclude about the models?

# Measures of influence (Leverage and Cook's distance)

We can generalise the idea from the last section, where we are interested in
how "influential" each data point is on the fit of the model.

There are two widely used numerical measures that have been designed for this
purpose.

## Leverage

Leverage is a measure of how "far away" a particular data point is from the
rest of the data set, based **only** on the predictors (in this case, just the
2012 medal counts).

The *leverage score*, $h_{ii}$, of the $i$th observation is a weighted distance
of $x_i$ to the mean of all $x_i$'s. The reason for the double index "$ii$" is
because the general definition---which we won't describe here---involves a
matrix of values, and the leverage score is on the diagonal of that matrix.

Intuitively, observations that are far from the mean of the predictors will
have higher **leverage**, i.e. they have potentially greater influence on the
fitted regression function. Changing the $y$ value of a high leverage point
even a little can really effect the fitted line.

A nice feature of the leverage score is that we can calculate them for all
points **without** having to fit all $n$ models (i.e., $n$ different
regressions where we leave out one observation).

In R, one way to extract the leverage values from the `lm()` output is to use
the `augment()` function from the `broom` package. For example:

```{r}
#| eval: false
# Add extra info for each observation relating to the model.
aug_oly <- augment(oly_lm) |> mutate(Country = oly$Country)

# Extract observations with large leverage.
aug_oly |>
  arrange(desc(.hat)) |>
  select(Country, .hat) |>
  head(15)
```

The `.hat` values are the leverage scores.

One rule of thumb for identifying high-leverage data points is to find those
with leverage scores that exceed $2p / n$, where p is the number of regression
coefficients and $n$ is the sample size.

***Exercise 18***

18. Find all high-leverage data points from the original fitted model, using
    this rule of thumb.

## Cook's distance

The above exercise identified 4 countries with high leverage. Although an
influential point will typically have high leverage, a high leverage point is
not necessarily an influential point. We would like something that measure
"influence" more directly.

*Cook's distance* (also known as Cook's $D$) measures the effect of deleting a
particular observation. Specifically, it "sums up" (in a particular way) the
changes in the fitted values if a given observation were deleted. It can be
expressed with the following formula:
$$D_i = \frac{e_i^2}{p \times \text{MSE}} \times \frac{h_{ii}}{(1-h_{ii})^2}$$
where $\text{MSE}$ is the mean squared error of the model
($\text{MSE} = \text{SSE} / (n - p)$).

Again, as a rule of thumb, we can look for any point with Cook's $D$ greater
than $2p / n$. This incorporates leverage and the residual, so looks for
influential observations "horizontally" and "vertically".

The `augment()` function we used above also extracts Cook's $D$, in the column
called `.cooksd`.

***Exercise 19***

19. Find all countries with a high Cook's $D$ value, using the rule of thumb.

## Visualising influence

The following plot combines various ideas we've explored into a single
diagnostic visualisation:

```{r}
#| eval: false
plot(oly_lm, 5)
```

The y-axis shows the residuals (like one of the original diagnostic plots we
used), while the x-axis shows leverage. Points further to the right are those
with higher leverage. The dashed curves show contours of Cook's distance; the
further a point is to the top-right or bottom-right, the higher the value of
Cook's $D$ for that point (meaning it has high leverage and a large residual).

From this plot we can see the three countries that have very high Cook's $D$:
they are numbered 1, 2 and 4 on the plot, corresponding to their index in `oly`
(the tibble to which we fitted the model). We can also see that Great Britain
has high leverage, but low Cook's $D$ because it has a small residual (it is
close to the fitted line).

# Improving the model

While the fit of the model to the data is excellent, in examining the residuals
we saw that the fit is highly sensitive to a few particular data points. That
means it could perform more poorly for prediction or description than we might
expect given the seemingly high $R^2$.

For example, this pair of Olympic games led to a particular set of tallies for
the high-ranking countries, but these could be much higher or lower "by
chance", compared to an arbitrary pair of games, and thus the fitted line could
be too high or too low. Because the model depends so heavily on a few data
points, it is more likely to be inaccurate "by chance". It is not benefiting
from being able to use all of the data sufficiently well.

We can change the model in various ways to try to improve it. Here are some
ideas:

-   **Transform the variables.** This is often a the first thing to try, if
    there is a clear way that a transformation could fix any defects in the
    model. However, note that the "error" will then be quantified on a
    different scale, which may or may not match the goals of your analysis.

-   **Add extra predictors.** This could work if we have other predictors
    available and they can ameliorate high leverage points (by making them less
    extreme in the expanded space of predictors).

-   **Remove some data.** This would usually entail changing the purpose of the
    analysis, since the data no longer represent the original population. It
    may or may not make sense given the overall goals of the analysis. It is
    often tempting to remove data, but typically this is not appropriate for
    most analyses; investigate other options first.

-   **Use a different model.** For example, the assumption of normally
    distributed errors might not be appropriate, and we can change it to a
    different distribution. Such models are beyond the scope of this unit.

In any given analysis, you need to decide which of these (if any) are
appropriate for your analysis needs.

For the purpose of today's exercise, let's explore the first of these options,
by trying the following transformations:

1.  Taking the logarithm of the medal counts.
2.  Taking the square root of the medal counts.

Transforming the data:

```{r}
#| eval: false
oly_trans <- oly |>
  mutate(log2012  = log(Total_2012),
         log2016  = log(Total_2016),
         sqrt2012 = sqrt(Total_2012),
         sqrt2016 = sqrt(Total_2016))
```

***Exercises 20--22***

20. Repeat the modelling using the log-transformed counts. Has it improved the
    fit of the model?

21. Repeat the modelling using the square-root-transformed counts. Has it
    improved the fit of the model?

22. Draw a scatter plot of the data, and overlay the three different models
    that we have fitted to it (not including the one where we have removed
    USA).

Hint: for the models with transformations, define your own function that
implements the formula for the fitted model, and add it to the scatter plot by
using `geom_function()`.
